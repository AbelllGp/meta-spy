# Home

![CLI1](https://github.com/DEENUU1/facebook-spy/blob/main/assets/v1_2/basic.gif?raw=true)
![CLI2](https://github.com/DEENUU1/facebook-spy/blob/main/assets/v1_2/search.gif?raw=true)
![CLI3](https://github.com/DEENUU1/facebook-spy/blob/main/assets/v1_2/full.gif?raw=true)


## About project

The project, known as "Facebook Spy," is a powerful and versatile tool designed to gather information from Facebook profiles, pages and search results. 

The project is under constant development to improve its functionality and address any issues that may arise.

One notable limitation of the tool is that some scrapers may not work correctly for profiles with default account IDs in the URL, such as "https://www.facebook.com/profile.php?id=100063142210972." These scrapers include work and education history, contact data, visited places, family members, recent places, reviews, and likes. However, this issue does not occur for profiles with custom IDs in the URL, like "https://www.facebook.com/zuck.".

## Key Features
- Login - Perform a secure two-step verification process to log in to Facebook account with enhanced security settings or log in to Facebook account using the default login method.
- Scrape details like: friend list, images, recent places, videos, reels, reviews, posts, likes, groups, events etc.
- Local web application - User is able to run a loval web application to browse scraped data, create notes for specified Person object and easly search details in web.
- Video downloader - download all scraped videos from facebook account or just by add passed url.
- Create a graphical representation of connections between different Facebook profiles based on their friendships.
- Utilize a free open-source language model to generate a concise summary of a Facebook user's infromation based on the scraped data.
- Initiate a friend crawler for a specified Facebook account. Gether data about friends and contunue the process for subsequent users in the queue.
- Save scraped data for a specified Facebook user to PDF file for documentation and anylysis.
- Running scrapers in parallel to speed up the data collection process
- Save all scraped data to JSON files and database 

        